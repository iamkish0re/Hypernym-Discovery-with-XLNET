{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k-E8zSHjUt7n"
   },
   "source": [
    "# Hypernym Discovery using XLNet language model\n",
    "Project work done for the course of Natural Language Processing for the Fall of 2023."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aqLo0l8ZW150",
    "outputId": "3e4b3450-f622-4519-de7f-add318d0e899",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install sentencepiece\n",
    "!pip install simpletransformers\n",
    "!pip install pytorch-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3JarDcOZnY4m"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import XLNetTokenizer, XLNetLMHeadModel\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "import datetime\n",
    "import os\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fhRuxTHE8--t"
   },
   "outputs": [],
   "source": [
    "# Create Variables for setting some basic parameters for training\n",
    "PROJECT_ROOT = os.getcwd()\n",
    "TRAINING_DATA_FILE=\"combined-preprocessed.data.txt\"\n",
    "TRAINING_GOLD_FILE=\"combined-preprocessed.gold.txt\"\n",
    "PREPROCESSED_TRAINING_DATA_PATH=PROJECT_ROOT+\"/SemEval2018-Task9/preprocessed/training/\"\n",
    "SAVE_MODEL_PATH=PROJECT_ROOT+\"/saved-models/\"\n",
    "BATCH_SIZE=4\n",
    "EPOCH_PER_TOKENSIZE={1:15,2:15,3:5}\n",
    "MODEL_SAVE_PREFIX=\"r1-all-xlnet-large-b32\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LIZyTTBaZ0MI"
   },
   "outputs": [],
   "source": [
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "\n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataframe(data_file, gold_file):\n",
    "    '''\n",
    "    Function to load the data onto a dataframe using pandas\n",
    "    '''\n",
    "    data_df = pd.read_csv(data_file, header=None, index_col=False)\n",
    "    label_df = pd.read_csv(gold_file, header=None, index_col=False)\n",
    "    return data_df, label_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jttfdSlmeaCK"
   },
   "outputs": [],
   "source": [
    "def tokenize_and_pad(input_data,output_data):\n",
    "  '''\n",
    "  Function that takes the input data and gold standard output data and tokenizes them for trianing using the XLNet model\n",
    "  '''\n",
    "\n",
    "  # We use the tokenizer that was used in the pretrainng of the XLNet model for tokenize our data as well.\n",
    "  tokenizer = XLNetTokenizer.from_pretrained('xlnet-large-cased')\n",
    "\n",
    "  label_list=[str(x[0]) for x in output_data.values ]\n",
    "  label_list=[(tokenizer.encode(x, add_special_tokens=False)) for x in label_list ]\n",
    "\n",
    "\n",
    "  PADDING_TEXT=\"\"\"The voice of Nicholas's young son, Tsarevich Alexei Nikolaevich, narrates the remainder of the story.A pamphlet with photos and comments from the journals kept by the students added to the display. <eod> <eos>\"\"\"\n",
    "  input_ids_list=[PADDING_TEXT +\" \"+ str(x[0]) +\" is a \" for x in input_data.values ]\n",
    "\n",
    "  label_list_dictionary={}\n",
    "  input_ids_dictionary={}\n",
    "\n",
    "  i=0\n",
    "  while(i<len(label_list)):\n",
    "    if(len(label_list[i])>3):\n",
    "      del label_list[i]\n",
    "      del input_ids_list[i]\n",
    "    else:\n",
    "      temp_input_ids=input_ids_list[i]+(len(label_list[i])*\"<mask>\")\n",
    "      temp_input_ids=tokenizer.encode(temp_input_ids, add_special_tokens=True)\n",
    "      if(label_list_dictionary.get(len(label_list[i])) is not None):\n",
    "        label_list_dictionary[len(label_list[i])].append(label_list[i])\n",
    "        input_ids_dictionary[len(label_list[i])].append(temp_input_ids)\n",
    "      else:\n",
    "        label_list_dictionary[len(label_list[i])]=[label_list[i]]\n",
    "        input_ids_dictionary[len(label_list[i])]=[temp_input_ids]\n",
    "      i=i+1\n",
    "\n",
    "  input_ids_attention_mask_dictionary={}\n",
    "  for i in label_list_dictionary.keys():\n",
    "    input_ids_dictionary[i] = pad_sequences(input_ids_dictionary[i],  dtype='long', value=5, padding='pre')\n",
    "    input_ids_dictionary[i]=torch.tensor(input_ids_dictionary[i])\n",
    "    input_ids_attention_mask_list = [[int(token_id !=5) for token_id in word] for word in input_ids_dictionary[i]]\n",
    "    input_ids_attention_mask=torch.tensor(input_ids_attention_mask_list)\n",
    "    input_ids_attention_mask_dictionary[i]=input_ids_attention_mask\n",
    "    label_list_dictionary[i]=torch.tensor(label_list_dictionary[i])\n",
    "  return input_ids_dictionary,label_list_dictionary,input_ids_attention_mask_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6zl5pZ3iNzLs"
   },
   "outputs": [],
   "source": [
    "def create_dataloader(input_id_list,label_list,attention_mask_list,batch_size):\n",
    "  '''\n",
    "  Function that creates a dataloader for the input_ids, labels, and the attention_mask\n",
    "  '''\n",
    "\n",
    "  #We are using a 90:10 split for training : validation split\n",
    "  train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_id_list, label_list,\n",
    "                                                              random_state=2018, test_size=0.1)\n",
    "  train_masks, validation_masks= train_test_split(attention_mask_list,random_state=2018, test_size=0.1)\n",
    "\n",
    "  train_inputs = torch.tensor(train_inputs)\n",
    "  validation_inputs = torch.tensor(validation_inputs)\n",
    "\n",
    "  train_labels = torch.tensor(train_labels)\n",
    "  validation_labels = torch.tensor(validation_labels)\n",
    "\n",
    "  train_masks = torch.tensor(train_masks)\n",
    "  validation_masks = torch.tensor(validation_masks)\n",
    "  # Create the DataLoader for our training set.\n",
    "  train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "  train_sampler = RandomSampler(train_data)\n",
    "  train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "  # Create the DataLoader for our validation set.\n",
    "  validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "  validation_sampler = SequentialSampler(validation_data)\n",
    "  validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n",
    "\n",
    "  return train_dataloader,validation_dataloader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TvyHVXCPZ3od"
   },
   "source": [
    "## A). Loading the preprocessed dataset.\n",
    "\n",
    "*  We load the preprocessed dataset from our folder and tokenize, pad and generate masks for the input hyponyms. We also split the data based on the number of tokens of the label hypernym as well to train them seperately.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tnTuyAlvoTuG"
   },
   "outputs": [],
   "source": [
    "#Import the preprocessed hyponym-hypernym pairs\n",
    "input_hyponym_df,output_hypernym_df=load_dataframe(PREPROCESSED_TRAINING_DATA_PATH+TRAINING_DATA_FILE,PREPROCESSED_TRAINING_DATA_PATH+TRAINING_GOLD_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ygp8ArSOiTht"
   },
   "outputs": [],
   "source": [
    "# Get the input, label and input attention mask from the preprocessed data. The input,label and mask is grouped with the size of the number of tokens the label(hypernym) had.\n",
    "input_ids_dictionary,label_list_dictionary,input_ids_attention_mask_dictionary=tokenize_and_pad(input_data=input_hyponym_df,output_data=output_hypernym_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B). Download and load the pretrained XLNet-Large model from HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1O45lpupjCVQ"
   },
   "outputs": [],
   "source": [
    "model = XLNetLMHeadModel.from_pretrained('xlnet-large-cased')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(),lr = 2e-5,eps = 1e-8 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C). Fine Tuning the model for our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O9DJaiQ8jJlx"
   },
   "outputs": [],
   "source": [
    "# \n",
    "token_size_list=[x for x in input_ids_dictionary.keys()]\n",
    "token_size_list.sort(reverse=True)\n",
    "for label_token_size in token_size_list:\n",
    "    \n",
    "  train_dataloader,validation_dataloader=create_dataloader(input_ids_dictionary[label_token_size],\n",
    "                                                           label_list_dictionary[label_token_size],\n",
    "                                                           input_ids_attention_mask_dictionary[label_token_size],\n",
    "                                                           BATCH_SIZE)\n",
    "\n",
    "  epochs = EPOCH_PER_TOKENSIZE[label_token_size]\n",
    "  total_steps = len(train_dataloader) * epochs\n",
    "  scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                              num_warmup_steps = 0,\n",
    "                                              num_training_steps = total_steps)\n",
    "  seed_val = 42\n",
    "  random.seed(seed_val)\n",
    "  np.random.seed(seed_val)\n",
    "  torch.manual_seed(seed_val)\n",
    "  torch.cuda.manual_seed_all(seed_val)\n",
    "    \n",
    "  loss_values = []\n",
    "  for epoch_i in range(0, epochs):\n",
    "\n",
    "      print(\"\")\n",
    "      print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "      print('Training...')\n",
    "\n",
    "      t0 = time.time()\n",
    "      total_loss = 0\n",
    "      model.train()\n",
    "\n",
    "      # For each batch of training data...\n",
    "      for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "          # Update progress every 20 batches\n",
    "          if step % 20 == 0 and not step == 0:\n",
    "              # Calculate elapsed time in minutes.\n",
    "              elapsed = format_time(time.time() - t0)\n",
    "\n",
    "              # Report progress.\n",
    "              print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "          # Unpack this training batch from our dataloader.\n",
    "          #\n",
    "          # As we unpack the batch, we'll also copy each tensor to the GPU using the\n",
    "          # `to` method.\n",
    "          #\n",
    "          # `batch` contains three pytorch tensors:\n",
    "          #   [0]: input ids\n",
    "          #   [1]: attention masks\n",
    "          #   [2]: labels\n",
    "\n",
    "          b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "          # We create the target mapping tensor and the perm_mask tensor based on the size of the label tokens\n",
    "          \n",
    "          targets=[(-2-i) for i in range(label_token_size,0,-1)] # This will be the indices of the masks that will be present in the input_ids\n",
    "          perm_mask_batch = torch.zeros((b_input_ids.shape[0], b_input_ids.shape[1], b_input_ids.shape[1]), dtype=torch.float)\n",
    "          perm_mask_batch[:, :, targets] = 1.0\n",
    "          target_mapping_batch = torch.zeros((b_input_mask.shape[0], len(targets), b_input_ids.shape[1]), dtype=torch.float)\n",
    "          for i in range(len(targets)):\n",
    "            target_mapping_batch[:, i, targets[i]] = 1.0\n",
    "\n",
    "          b_input_ids = b_input_ids.to(device)\n",
    "          b_input_mask = b_input_mask.to(device)\n",
    "          b_labels = b_labels.to(device)\n",
    "          perm_mask_batch_tensor=perm_mask_batch.to(device)\n",
    "          target_mapping_batch_tensor=target_mapping_batch.to(device)\n",
    "\n",
    "          model.zero_grad()\n",
    "\n",
    "          outputs = model(input_ids=b_input_ids,attention_mask=b_input_mask,labels=b_labels,perm_mask=perm_mask_batch_tensor, target_mapping=target_mapping_batch_tensor)\n",
    "\n",
    "          loss = outputs[0]\n",
    "\n",
    "          total_loss += loss.item()\n",
    "\n",
    "          loss.backward()\n",
    "\n",
    "          torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "          optimizer.step()\n",
    "\n",
    "          scheduler.step()\n",
    "\n",
    "      # Calculate the average loss over the training data.\n",
    "      avg_train_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "      # Store the loss value for plotting the learning curve.\n",
    "      loss_values.append(avg_train_loss)\n",
    "\n",
    "      print(\"\")\n",
    "      print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "      print(\"  Training epoch took: {:}\".format(format_time(time.time() - t0)))\n",
    "\n",
    "\n",
    "\n",
    "      # ========================================\n",
    "      #               Validation\n",
    "      # ========================================\n",
    "      # After the completion of each training epoch, measure our performance on\n",
    "      # our validation set.\n",
    "\n",
    "      print(\"\")\n",
    "      print(\"Running Validation...\")\n",
    "      t0 = time.time()\n",
    "      model.eval()\n",
    "      tmp_validation_loss, validation_loss = 0, 0\n",
    "      nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "      # Evaluate data for one epoch\n",
    "      for batch in validation_dataloader:\n",
    "          batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "          b_input_ids, b_input_mask, b_labels = batch\n",
    "          b_input_ids = b_input_ids.to(device)\n",
    "          b_input_mask = b_input_mask.to(device)\n",
    "          b_labels = b_labels.to(device)\n",
    "          perm_mask_batch = torch.zeros((b_input_ids.shape[0], b_input_ids.shape[1], b_input_ids.shape[1]), dtype=torch.float)\n",
    "          perm_mask_batch[:, :, targets] = 1.0\n",
    "          target_mapping_batch = torch.zeros((b_input_mask.shape[0], len(targets), b_input_ids.shape[1]), dtype=torch.float)\n",
    "\n",
    "          for i in range(len(targets)):\n",
    "            target_mapping_batch[:, i, targets[i]] = 1.0\n",
    "\n",
    "          perm_mask_batch_tensor=perm_mask_batch.to(device)\n",
    "          target_mapping_batch_tensor=target_mapping_batch.to(device)\n",
    "\n",
    "          with torch.no_grad():\n",
    "              outputs = model(input_ids=b_input_ids,attention_mask=b_input_mask,labels=b_labels,perm_mask=perm_mask_batch_tensor, target_mapping=target_mapping_batch_tensor)\n",
    "\n",
    "          logits = outputs[0]\n",
    "          tmp_validation_loss=logits.cpu().item()\n",
    "\n",
    "          validation_loss += tmp_validation_loss\n",
    "\n",
    "          nb_eval_steps += 1\n",
    "\n",
    "      # Report the final accuracy for this validation run.\n",
    "      print(\"  Validation Loss: {0:.2f}\".format(validation_loss/nb_eval_steps))\n",
    "      print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
    "\n",
    "      if((epoch_i+1)%4==0):\n",
    "        model_save_name=SAVE_MODEL_PATH+MODEL_SAVE_PREFIX+str(epoch_i+1)+\"-v2-ts\"+str(label_token_size)\n",
    "        torch.save(model,model_save_name)\n",
    "        print(\"Model saved in :\"+model_save_name)\n",
    "\n",
    "  print(\"\")\n",
    "  print(\"Training complete for \"+str(label_token_size)+\" sized labels.\")\n",
    "\n",
    "model_save_name=SAVE_MODEL_PATH+MODEL_SAVE_PREFIX+\"-final\"\n",
    "torch.save(model,model_save_name)\n",
    "print(\"Final Model saved in :\"+model_save_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WIF6btK7YFLL"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "V100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
